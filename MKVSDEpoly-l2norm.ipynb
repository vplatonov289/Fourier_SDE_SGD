{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a00fece",
   "metadata": {},
   "source": [
    "## SGD setup (using $L^2$ norm) for a MKVSDE with polynomial drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ee68b",
   "metadata": {},
   "source": [
    "Consider a McKean-Vlasov (MKV) SDE of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{d}X_t = \\bigl(\\rho X_t + \\mathbb{E}[X_t] - X_t \\mathbb{E}[X^2_t] \\bigr) \\textrm{d} t + X_t \\, \\textrm{d} W_t, \\quad X_0 = x_0.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca4404",
   "metadata": {},
   "source": [
    "We approximate the solution $X_t, t \\in [0,T]$ by using a polynomial approximation for the function $\\mathbb{E}[X_t]$ and $\\mathbb{E}[X^2_t)].$ In particular we use $\\widehat{\\gamma}_{1,k}, k=1,\\ldots,n,$ as the value of $\\mathbb{E}[X_t]$ for $t \\in \\{t_1, \\ldots, t_n\\},$ and $\\widehat{\\gamma}_{2,k}, k=1,\\ldots,n,$ as the value of $\\mathbb{E}[X^2_t]$ for $t \\in \\{t_1, \\ldots, t_n\\},$ where the values $\\{t_1, \\ldots, t_n\\},$ are either Chebychev or Lagrange nodes in the time interval $[0, T].$ The process with polynomial function approximation is generated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{d}Z_t = \\bigl(\\beta Z_t + \\sum^{n}_{k=1} \\widehat{\\gamma}_{1,k} \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  -  Z_t \\sum^{n}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\bigr) \\textrm{d} t + Z_t \\, \\textrm{d} W_t, \\quad Z_0 = x_0.\n",
    "\\end{align}\n",
    "\n",
    "The gradient processes $\\xi^k_{1,t}:= \\partial_{\\widehat{\\gamma}_{1,k}}Z_t$ and $\\xi^k_{2,t}:= \\partial_{\\widehat{\\gamma}_{2,k}}Z_t$ for $k = 1, \\ldots, n,$ are given as\n",
    "\\begin{align}\n",
    "    \\textrm{d}\\xi^k_{1,t} &= \\bigl(\\beta \\xi^k_{1,t} + \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}-\\xi^k_{1,t} \\sum^{n}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}\\bigr) \\textrm{d} t + \\xi^k_{1,t}\\, \\textrm{d} W_t, \\quad \\xi^k_{1,0} = 0,\\\\\n",
    "    \\textrm{d}\\xi^k_{2,t} &= \\bigl(\\beta \\xi^k_{2,t} -\\xi^k_{2,t} \\sum^{n}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j} - Z_t \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}\\bigr) \\textrm{d} t  + \\xi^k_{2,t}\\, \\textrm{d} W_t, \\quad \\xi^k_{2,0} = 0.\n",
    "\\end{align}\n",
    "    \n",
    "We disretise the time interval $[0, T]$ uniformly into $M + 1$ steps, and insert Chebychev nodes to create a final time grid with $M+n$ (at most) intermediate points to generate discretised path of $Z$ and gradient processes. Our aim is to find the fixed-point of the map $\\widehat{\\Psi}^{(M+n)}$ defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{\\Psi}^{(M+n)}(\\widehat{\\gamma}):= \\mathcal{P}\\Bigl(\\Psi\\bigl(\\mathcal{L}(\\widehat{\\gamma})\\bigr)\\Bigr).\n",
    "\\end{align}\n",
    "\n",
    "In the above, we denote $\\mathcal{L}$ as the lifting operator and $\\mathcal{P}$ as the projection operator. In the linear MKV-SDE considered here, we have $\\Psi\\bigl(\\mathcal{L}(\\widehat{\\gamma})\\bigr)(t) = \\bigl(\\mathbb{E}[Z^{\\mathcal{L}(\\widehat{\\gamma})}_t], \\mathbb{E}[{Z^{\\mathcal{L}(\\widehat{\\gamma})}_t}^2]\\bigr).$ We compute the fixed-point by solving the following: \n",
    "\\begin{align}\n",
    "\\min_{\\widehat{\\gamma}} \\big| \\mathcal{L}\\Bigl(\\widehat{\\Psi}^{(M+n)}(\\widehat{\\gamma})\\Bigr) - \\mathcal{L}(\\widehat{\\gamma})\\big|^2_{L^2([0,T])}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa81c0",
   "metadata": {},
   "source": [
    "More precisely, we write $\\widehat{\\gamma} = \\{\\widehat{\\gamma}_{1,1}, \\ldots, \\widehat{\\gamma}_{1,n},\\widehat{\\gamma}_{2,1}, \\ldots, \\widehat{\\gamma}_{2,n}\\}.$ Then, \n",
    "\\begin{align}\n",
    "(\\mathcal{L}(\\widehat{\\gamma}))_{l=1,2}(t) = \\sum^{n}_{k=1} \\widehat{\\gamma}_{l,k} \\prod_{\\substack{ 1\\leq j \\leq n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}.\n",
    "\\end{align}\n",
    "The above notation gives us \n",
    "\\begin{align}\n",
    " \\big( \\mathcal{L} \\widehat{\\Psi}^{(M+n)}(\\widehat\\gamma) - \\mathcal{L}\\widehat\\gamma \\big)_1(t) &= \\mathbb{E}\\bigg[ \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k \\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big( Z^{\\mathcal{L}\\widehat \\gamma}_{t_k}  -  \\widehat\\gamma_{1,k} \\Big) \\bigg],\\\\\n",
    " \\big( \\mathcal{L} \\widehat{\\Psi}^{(M+n)}(\\widehat\\gamma) -  \\mathcal{L}\\widehat\\gamma \\big)_2(t) &= \\mathbb{E}\\bigg[ \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k \\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big( (Z^{\\mathcal{L}\\widehat \\gamma}_{t_k})^2  -  \\widehat\\gamma_{2,k} \\Big) \\bigg],\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a7160",
   "metadata": {},
   "source": [
    "In order to perform stochastic gradient descent, we need to define the following.\n",
    "\n",
    "\\begin{align}\n",
    "F_{1}(\\hat\\gamma;t, W) &:= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big(Z^{\\mathcal{L}\\widehat{\\gamma}}_{t_k} - \\widehat{\\gamma}_{1,k} \\Big),\\\\\n",
    "F_{2}(\\hat\\gamma;t, W) &:= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big((Z^{\\mathcal{L}\\widehat{\\gamma}}_{t_k})^2 - \\widehat{\\gamma}_{2,k} \\Big),\\\\\n",
    "\\partial_{\\hat\\gamma_{1,m}} F_{1}(\\hat\\gamma;t, W) &= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(\n",
    "\\xi^{m}_{1,t_{k}} (\\hat\\gamma) - \\delta_{k}(m) \\Big), \\quad m = 1, \\ldots, n,\\\\\n",
    "\\partial_{\\hat\\gamma_{1,m}} F_{2}(\\hat\\gamma;t, W) &= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(\n",
    "2 Z^{\\mathcal{L}(\\hat \\gamma)}_{t_k} \\cdot \\xi^{m}_{1,t_{k}} (\\hat\\gamma)\\Big), \\quad m = 1, \\ldots, n,\\\\\n",
    "\\partial_{\\hat\\gamma_{2,m}} F_{1}(\\hat\\gamma;t, W) &= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(\\xi^{m}_{2,t_{k}} (\\hat\\gamma)\\Big), \\quad m= 1, \\ldots, n,\\\\\n",
    "\\partial_{\\hat\\gamma_{2,m}} F_{2}(\\hat\\gamma;t, W) &= \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(\n",
    "2 Z^{\\mathcal{L}(\\hat \\gamma)}_{t_k} \\cdot \\xi^{m}_{2,t_{k}} (\\hat\\gamma) - \\delta_{k}(m) \\Big), \\quad m= 1, \\ldots, n.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecda27d",
   "metadata": {},
   "source": [
    "The Jacobian matrix is then given as \n",
    "    \\begin{equation}\n",
    "        J_{\\widehat{\\gamma}}F = \\begin{bmatrix}\n",
    "                                \\nabla^\\top F_1(\\widehat{\\gamma}, W) \\\\\n",
    "                                \\nabla^\\top F_{2}(\\widehat{\\gamma}, W)\n",
    "                                \\end{bmatrix}\n",
    "                                = \\begin{bmatrix}\n",
    "                                    \\frac{\\partial F_1}{\\partial \\widehat{\\gamma}_{1,1}} \\ldots \\frac{\\partial F_1}{\\partial \\widehat{\\gamma}_{2,n}}\\\\\n",
    "                                    \\frac{\\partial F_{2K}}{\\partial \\widehat{\\gamma}_{1,1}} \\ldots \\frac{\\partial F_{2K}}{\\partial \\widehat{\\gamma}_{2,n}}\\\\\n",
    "                                    \\end{bmatrix}\n",
    "                               = \\begin{bmatrix}\n",
    "                                   \\mathcal{L}\\bigl(\\xi^1_{1} - \\delta(1)\\bigr) & \\mathcal{L}\\bigl(\\xi^2_{1} - \\delta(2)\\bigr) & \\ldots & \\mathcal{L}\\bigl(\\xi^n_{1} - \\delta(n)\\bigr) & \\mathcal{L}\\bigl(\\xi^1_{2}\\bigr) & \\ldots & \\mathcal{L}\\bigl(\\xi^n_{2}\\bigr)  \\\\\n",
    "                                   \\mathcal{L}\\bigl(2 Z \\xi^1_{1}\\bigr) & \\mathcal{L}\\bigl(2 Z \\xi^2_{1} \\bigr) & \\ldots & \\mathcal{L}\\bigl(2 Z \\xi^n_{1}\\bigr) & \\mathcal{L}\\bigl(2 Z \\xi^1_{2} - \\delta(1)\\bigr) & \\ldots & \\mathcal{L}\\bigl(2 Z\\xi^n_{2}- \\delta(n)\\bigr) \n",
    "                               \\end{bmatrix}.\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797e96b",
   "metadata": {},
   "source": [
    "To calculate the SGD step, we compute\n",
    "\\begin{align}\n",
    "{\\bf v} := 2 \\int_0^T  F({\\bf w};t,W) \\cdot J_w F({\\bf w};t, \\tilde{W} ) \\mathrm{d} t.\n",
    "\\end{align}\n",
    "In the above, we have\n",
    "\\begin{equation}\n",
    "F = \\begin{bmatrix}\n",
    "F_1(\\widehat{\\gamma}, W) & F_{2}(\\widehat{\\gamma}, W)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3643e4b",
   "metadata": {},
   "source": [
    "Thus, we have for $m = 1, \\ldots, n$\n",
    "\\begin{align}\n",
    "{\\bf v}_{m} &= 2 \\int^T_0 F_1(\\widehat{\\gamma}, W) \\mathcal{L}\\bigl(\\xi^m_{1} - \\delta(m)\\bigr) + F_{2}(\\widehat{\\gamma}, W)\\mathcal{L}\\bigl(2 Z \\xi^m_{1}\\bigr) \\mathrm{d}t\\\\\n",
    "&= 2 \\int^T_0 \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big(Z^{\\mathcal{L}\\widehat{\\gamma}}_{t_k}(W) - \\widehat{\\gamma}_{1,k} \\Big) \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(\n",
    "\\xi^{m}_{1,t_{k}} (\\tilde W) - \\delta_{k}(m) \\Big) \\mathrm{d} t \\\\\n",
    "&+ 2 \\int^T_0 \\sum_{k=1}^n \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)   \\Big((Z^{\\mathcal{L}\\widehat{\\gamma}}_{t_k})^2(W) - \\widehat{\\gamma}_{2,k} \\Big)  \\sum_{k=1}^n  \\Bigg(   \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\Bigg)  \\Big(2 Z^{\\mathcal{L}(\\hat \\gamma)}_{t_k}(\\tilde W) \\cdot \\xi^{m}_{1,t_{k}} (\\tilde W)\\Big) \\mathrm{d} t\\\\\n",
    "&= 2 \\sum_{i_1,i_2=0}^{n} \\Big(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{i_1}}(W) - \\widehat\\gamma_{1,i_1}  \\Big) \\, \n",
    "\\Big(\\xi^{m}_{1,t_{{i_2}}} (\\tilde W) - \\delta_{i_2}(m) \\Big) \\beta_{i_1,i_2}  + 2 \\sum_{i_1,i_2=0}^{n} \\Big((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{i_1}})^2(W) - \\widehat\\gamma_{2,i_1}  \\Big) \\, \n",
    "\\Big(2 Z^{\\mathcal{L}(\\hat \\gamma)}_{t_{i_2}}(\\tilde W) \\cdot\\xi^{m}_{1,t_{{i_2}}} (\\tilde W) \\Big) \\beta_{i_1,i_2},\n",
    "\\end{align}\n",
    "with $\\beta_{i_1,i_2}$ given as \n",
    "\\begin{align}\n",
    "   \\beta_{i_1,i_2} =\\int_0^T  \\bigg( \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ i_1\\neq j}} \\frac{t-t_j}{t_{i_1} - t_j}\\bigg) \\bigg(  \\prod_{\\substack{ 1\\leqslant j \\leqslant n \\\\ i_2\\neq j}} \\frac{t-t_j}{t_{i_2} - t_j} \\bigg) \\mathrm{d} t.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71eb48",
   "metadata": {},
   "source": [
    "For $m = n+1, \\ldots, 2n,$ we have \n",
    "\\begin{align}\n",
    "{\\bf v}_{m} &= 2 \\int^T_0 F_1(\\widehat{\\gamma}, W) \\mathcal{L}\\bigl(\\xi^{m-n}_{2}\\bigr) + F_{2}(\\widehat{\\gamma}, W)\\mathcal{L}\\bigl(2 Z \\xi^{m-n}_{2} - \\delta(m-n)\\bigr) \\mathrm{d}t\\\\\n",
    "&= 2 \\sum_{i_1,i_2=0}^{n} \\Big(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{i_1}}(W) - \\widehat\\gamma_{1,i_1}  \\Big) \\, \n",
    "\\Big(\\xi^{m-n}_{2,t_{{i_2}}} (\\tilde W) \\Big) \\beta_{i_1,i_2}  + 2 \\sum_{i_1,i_2=0}^{n} \\Big((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{i_1}})^2(W) - \\widehat\\gamma_{2,i_1}  \\Big) \\, \n",
    "\\Big(2 Z^{\\mathcal{L}(\\hat \\gamma)}_{t_{i_2}}(\\tilde W) \\cdot\\xi^{m-n}_{2,t_{{i_2}}} (\\tilde W)  - \\delta_{i_2}(m-n)\\Big) \\beta_{i_1,i_2}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6295b",
   "metadata": {},
   "source": [
    "The vector $\\mathbf{v}$ can be computed by the following matrix operations.\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\n",
    "Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}}(W) - \\widehat\\gamma_{1,1} & \\ldots n-2 \\ldots & Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}}(W) - \\widehat\\gamma_{1,1} & (Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}})^2(W) - \\widehat\\gamma_{2,1} & \\ldots n-2 \\ldots &(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}})^2(W) - \\widehat\\gamma_{2,1} \\\\\n",
    "Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}}(W) - \\widehat\\gamma_{1,2} & \\ldots n-2 \\ldots & Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}}(W) - \\widehat\\gamma_{1,2} & (Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}})^2(W) - \\widehat\\gamma_{2,2} & \\ldots n-2 \\ldots &(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}})^2(W) - \\widehat\\gamma_{2,2} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}}(W) - \\widehat\\gamma_{1,n} & \\ldots n-2 \\ldots & Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}}(W) - \\widehat\\gamma_{1,n} & (Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}})^2(W) - \\widehat\\gamma_{2,n} & \\ldots n-2 \\ldots &(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}})^2(W) - \\widehat\\gamma_{2,n}\n",
    "\\end{bmatrix}\\\\\n",
    "& \\cdot \\begin{bmatrix}\n",
    "\\beta_{1,1} & \\beta_{1,2} & \\beta_{1,3} & \\ldots & \\beta_{1,n} & \\beta_{1,1} & \\beta_{1,2} & \\ldots & \\beta_{1,n}\\\\\n",
    "\\beta_{2,1} & \\beta_{2,2} & \\beta_{2,3} & \\ldots & \\beta_{2,n} & \\beta_{2,1} & \\beta_{2,2} & \\ldots & \\beta_{2,n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\beta_{n,1} & \\beta_{n,2} & \\beta_{n,3} & \\ldots & \\beta_{n,n} & \\beta_{n,1} & \\beta_{n,2} & \\ldots & \\beta_{n,n}\n",
    "\\end{bmatrix}\\\\\n",
    "&\\times \\begin{bmatrix}\n",
    "\\xi^1_{1,t_{1}} - 1 & \\xi^2_{1,t_{1}} & \\ldots & \\xi^1_{2,t_{1}} & \\xi^2_{2,t_{1}} & \\ldots \\\\\n",
    "\\xi^1_{1,t_{2}} & \\xi^2_{1,t_{2}} - 1 & \\ldots & \\xi^1_{2,t_{2}} & \\xi^2_{2,t_{2}} & \\ldots \\\\\n",
    "\\xi^1_{1,t_{3}} & \\xi^2_{1,t_{3}} & \\ldots & \\ldots & \\ldots & \\ldots \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots \\\\\n",
    "\\xi^1_{1,t_{n}} & \\xi^2_{1,t_{n}} & \\ldots & \\xi^1_{2,t_{n}} & \\xi^2_{2,t_{n}} & \\ldots\\\\\n",
    "2 Z_{t_1} \\xi^1_{1,t_{1}} & 2 Z_{t_1} \\xi^2_{1,t_{1}} & \\ldots & 2 Z_{t_1} \\xi^1_{2,t_{1}} - 1 & 2 Z_{t_1} \\xi^2_{2,t_{1}} & \\ldots \\\\\n",
    "\\ldots & \\ldots & \\ldots & 2 Z_{t_2} \\xi^1_{2,t_{2}} & 2 Z_{t_2} \\xi^2_{2,t_{2}} - 1 & \\ldots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots\\\\\n",
    "2 Z_{t_n} \\xi^1_{1,t_{n}} & 2 Z_{t_n} \\xi^2_{1,t_{n}} & \\ldots & 2 Z_{t_n} \\xi^1_{2,t_{n}} & 2 Z_{t_n} \\xi^2_{2,t_{n}} & \\ldots\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4269d86",
   "metadata": {},
   "source": [
    "=\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}}(W)- \\widehat\\gamma_{1,1}\\bigr) \\sum^n_{i_2 = 1} \\beta_{1,i_{2}} (\\xi^1_{1,t_{i_2}} - \\delta_{i_2}(1)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}})^2(W) - \\widehat\\gamma_{2,1} \\bigr)\\sum^n_{i_2 = 1} 2 \\beta_{1,i_{2}} Z_{t_{i_2}}(\\tilde W)\\xi^1_{1,t_{i_2}} (\\tilde W) & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}}(W)- \\widehat\\gamma_{1,1}\\bigr) \\sum^n_{i_2 = 1} \\beta_{1,i_{2}} (\\xi^2_{1,t_{i_2}} - \\delta_{i_2}(2)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}})^2(W) - \\widehat\\gamma_{2,1} \\bigr)\\sum^n_{i_2 = 1} 2 \\beta_{1,i_{2}}  Z_{t_{i_2}}(\\tilde W)\\xi^2_{1,t_{i_2}} (\\tilde W) & \\ldots & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}}(W)- \\widehat\\gamma_{1,1}\\bigr) \\sum^n_{i_2 = 1} \\beta_{1,i_{2}}  \\xi^1_{2,t_{i_2}}(\\tilde W) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{1}})^2(W) - \\widehat\\gamma_{2,1} \\bigr)\\sum^n_{i_2 = 1} \\beta_{1,i_{2}} \\bigl(2 Z_{t_{i_2}}(\\tilde W)\\xi^1_{2,t_{i_2}} (\\tilde W) - \\delta_{i_2}(1) \\bigr)& \\ldots \\\\ \n",
    "\\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}}(W)- \\widehat\\gamma_{1,2}\\bigr) \\sum^n_{i_2 = 1} \\beta_{2,i_{2}} (\\xi^1_{1,t_{i_2}} - \\delta_{i_2}(1)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}})^2(W) - \\widehat\\gamma_{2,2} \\bigr)\\sum^n_{i_2 = 1} 2 \\beta_{2,i_{2}} Z_{t_{i_2}}(\\tilde W)\\xi^1_{1,t_{i_2}} (\\tilde W) & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}}(W)- \\widehat\\gamma_{1,2}\\bigr) \\sum^n_{i_2 = 1}\\beta_{2,i_{2}} (\\xi^2_{1,t_{i_2}} - \\delta_{i_2}(2)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}})^2(W) - \\widehat\\gamma_{2,2} \\bigr)\\sum^n_{i_2 = 1} 2\\beta_{2,i_{2}} Z_{t_{i_2}}(\\tilde W)\\xi^2_{1,t_{i_2}} (\\tilde W) & \\ldots & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}}(W)- \\widehat\\gamma_{1,2}\\bigr) \\sum^n_{i_2 = 1} \\beta_{2,i_{2}} \\xi^1_{2,t_{i_2}}(\\tilde W) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{2}})^2(W) - \\widehat\\gamma_{2,2} \\bigr)\\sum^n_{i_2 = 1} \\beta_{2,i_{2}}\\bigl(2 Z_{t_{i_2}}(\\tilde W)\\xi^1_{2,t_{i_2}} (\\tilde W) - \\delta_{i_2}(1) \\bigr)& \\ldots \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\ddots \\\\\n",
    "\\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}}(W)- \\widehat\\gamma_{1,n}\\bigr) \\sum^n_{i_2 = 1} \\beta_{n,i_{2}} (\\xi^1_{1,t_{i_2}} - \\delta_{i_2}(1)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}})^2(W) - \\widehat\\gamma_{2,n} \\bigr)\\sum^n_{i_2 = 1} 2 \\beta_{n,i_{2}}Z_{t_{i_2}}(\\tilde W)\\xi^1_{1,t_{i_2}} (\\tilde W) & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}}(W)- \\widehat\\gamma_{1,n}\\bigr) \\sum^n_{i_2 = 1} \\beta_{n,i_{2}}(\\xi^2_{1,t_{i_2}} - \\delta_{i_2}(2)) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}})^2(W) - \\widehat\\gamma_{2,n} \\bigr)\\sum^n_{i_2 = 1} 2 \\beta_{n,i_{2}}Z_{t_{i_2}}(\\tilde W)\\xi^2_{1,t_{i_2}} (\\tilde W) & \\ldots & \\bigl(Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}}(W)- \\widehat\\gamma_{1,n}\\bigr) \\sum^n_{i_2 = 1} \\beta_{n,i_{2}}\\xi^1_{2,t_{i_2}}(\\tilde W) + \\bigl((Z^{\\mathcal{L}\\widehat \\gamma}_{t_{n}})^2(W) - \\widehat\\gamma_{2,n} \\bigr)\\sum^n_{i_2 = 1} \\beta_{n,i_{2}} \\bigl(2 Z_{t_{i_2}}(\\tilde W)\\xi^1_{2,t_{i_2}} (\\tilde W) - \\delta_{i_2}(1) \\bigr)& \\ldots\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813ba64",
   "metadata": {},
   "source": [
    "Thus, if we take the sum of each column and multiply by scalar 2, we get $\\mathbf{v}$ as the resulting row vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778eb49",
   "metadata": {},
   "source": [
    "### Implementation of method with approximation using Chebychev nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dc38c",
   "metadata": {},
   "source": [
    "#### *Import relevant libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e646d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc # for TeX in plot labels\n",
    "from scipy.integrate import quad # to compute the L2-norm weights\n",
    "\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "## for Palatino and other serif fonts use:\n",
    "# rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "# rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f910e0",
   "metadata": {},
   "source": [
    "#### *Import plot libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fff3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1a30b",
   "metadata": {},
   "source": [
    "#### *Chebyvchev nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CC_nodes(a, b, n):\n",
    "    i = numpy.array(range(n))\n",
    "    x = numpy.cos((2 * i + 1) * pi /(2 * n))\n",
    "    z = 0.5 * (b-a) * x + 0.5 * (b + a)\n",
    "    return numpy.flip(z, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5f11d",
   "metadata": {},
   "source": [
    "#### *Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50  # number of discretisation steps\n",
    "n = 5  # number of Chebychev nodes\n",
    "x0 = 1 # starting value\n",
    "rho = 0.5 # constant in the SDE\n",
    "T = 0.5 # time period\n",
    "dt = T / M # time discretisation stepx\n",
    "max_iter = 5 * 10 ** 4 # number of iterations of SGD\n",
    "max_batch_iter = 5 * 10 ** 3 # number of iterations of batch SGD\n",
    "BATCH_SIZE = 200 # batch size\n",
    "\n",
    "lr = 0.05 # learning rate\n",
    "error_tol = 10 ** -5 # error tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0a15b",
   "metadata": {},
   "source": [
    "#### *Compute time grid and factors in polynomial approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39d102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspace = numpy.linspace(0, T, M + 1, dtype=float) # discretisation points\n",
    "tnodes = CC_nodes(0, T, n) # Chebychev nodes\n",
    "\n",
    "# insert unique Chebychev nodes in the time grid based on M values\n",
    "tspace = numpy.unique(numpy.concatenate((tspace, tnodes), axis=0))\n",
    "tgrid_values = len(tspace) # number of steps in the final time\n",
    "\n",
    "# identify the indices of Chebychev nodes in the time grid\n",
    "tnodes_idx = numpy.zeros(n, dtype=int)\n",
    "ctr = 0\n",
    "for j in range(tgrid_values):\n",
    "    if ctr < n: \n",
    "        if tnodes[ctr] == tspace[j]:\n",
    "            tnodes_idx[ctr] = j\n",
    "            ctr +=1 \n",
    "\n",
    "# tgrid_values product values for K different Chebychev nodes\n",
    "tspace_fact = numpy.ones((tgrid_values, n)) \n",
    "tnode_prod = numpy.zeros(n) # for computing the denominator in the factor\n",
    "\n",
    "# compute products in the denominator of polynomial approxmiation factor\n",
    "for k in range(n):\n",
    "    temp_prod = tnodes[k] - numpy.delete(tnodes, k)\n",
    "    tnode_prod[k] = numpy.prod(temp_prod)\n",
    "\n",
    "# compute time factors in the polynomial approxmiation\n",
    "for i in range(tgrid_values):\n",
    "    for k in range(n):\n",
    "        temp_prod = tspace[i] - numpy.delete(tnodes, k)\n",
    "        tspace_fact[i][k] =  numpy.prod(temp_prod) / tnode_prod[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795f3e4",
   "metadata": {},
   "source": [
    "#### *Compute weights for the $L^2$ norm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87e27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand(tnodes, idx1, idx2, x):\n",
    "    temp_prod1 = 1\n",
    "    temp_prod2 = 1\n",
    "    for i in range(n):\n",
    "        if i != idx1:\n",
    "            temp_prod1 = temp_prod1 * (x - tnodes[i]) / (tnodes[idx1] - tnodes[i])\n",
    "            \n",
    "        if i != idx2: \n",
    "            temp_prod2 = temp_prod2 * (x - tnodes[i]) / (tnodes[idx2] - tnodes[i])\n",
    "   \n",
    "    return temp_prod1 * temp_prod2\n",
    "\n",
    "normwt = numpy.zeros((n,n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        normwt[i][j] = quad(lambda x: integrand(tnodes, i, j, x), 0, T)[0]\n",
    "\n",
    "# Create a tiled matrix of size n x 2n with repetition of the weights\n",
    "normwt_mat = numpy.tile(normwt, (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b3cf7",
   "metadata": {},
   "source": [
    "#### *Define MKV SDE class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d7f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKVSDEpoly_approx():\n",
    "    \"\"\"\n",
    "    An MKVSDE class with 3 types of paths:\n",
    "        1. Underlying MKV path for computing the loss function with BM 1 (path)\n",
    "        2. An independent MKV path for computing the jacobian with BM 2 (path_grad)\n",
    "        3. Gradient process paths driven by BM 2 (grad)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tgrid_values, n):\n",
    "        super().__init__()\n",
    "        self.path = numpy.zeros(tgrid_values) # sample path\n",
    "        self.path_grad = numpy.zeros(tgrid_values) # sample path to be used for gradient process\n",
    "        self.grad = numpy.zeros((2, n, tgrid_values)) # gradient processes\n",
    "    \n",
    "    def generatepath(self, x0, gamma, beta, tgrid_values, tspace, dW):\n",
    "        self.path[0] = x0\n",
    "        self.path_grad[0] = x0\n",
    "        for i in range(tgrid_values-1):\n",
    "            dt = tspace[i+1] - tspace[i]\n",
    "            \n",
    "            self.path[i+1] = self.path[i]  +  beta * self.path[i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path[i] * dt ** 0.5 * dW[0][i]\n",
    "            \n",
    "            self.path_grad[i+1] = self.path_grad[i]  +  beta * self.path_grad[i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path_grad[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path_grad[i] * dt ** 0.5 * dW[1][i]\n",
    "            \n",
    "            self.grad[0,:,i+1] = self.grad[0,:,i]  + beta * self.grad[0,:,i] * dt \\\n",
    "                + tspace_fact[i] * dt \\\n",
    "                - self.grad[0,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "                + self.grad[0,:,i] * dt ** 0.5 * dW[1][i]\n",
    "            \n",
    "            self.grad[1,:,i+1] = self.grad[1,:,i] + beta * self.grad[1,:,i] * dt \\\n",
    "                - self.path_grad[i] * tspace_fact[i] * dt \\\n",
    "                - self.grad[1,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "                + self.grad[1,:,i] * dt ** 0.5 * dW[1][i]\n",
    "                \n",
    "class MKVSDEpolybatch_approx():\n",
    "    \"\"\"\n",
    "    An MKVSDE class that has a batch of size BATCH_SIZE with 3 types of paths:\n",
    "        1. Underlying MKV path for computing the loss function with BM 1 (path)\n",
    "        2. An independent MKV path for computing the jacobian with BM 2 (path_grad)\n",
    "        3. Gradient process paths driven by BM 2 (grad)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tgrid_values, n, BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.path = numpy.zeros((BATCH_SIZE, tgrid_values)) # sample path\n",
    "        self.path_grad = numpy.zeros((BATCH_SIZE, tgrid_values)) # sample path to be used for gradient process\n",
    "        self.grad = numpy.zeros((2, n, BATCH_SIZE, tgrid_values)) # gradient processes\n",
    "    \n",
    "    def generatebatch(self, x0, gamma, beta, tgrid_values, n, BATCH_SIZE, tspace, dW):\n",
    "        self.path[:,0] = x0\n",
    "        self.path_grad[:,0] = x0\n",
    "        for i in range(tgrid_values-1):\n",
    "            dt = tspace[i+1] - tspace[i]\n",
    "            \n",
    "            self.path[:,i+1] = self.path[:,i]  +  beta * self.path[:,i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path[:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path[:,i] * dt ** 0.5 * dW[0,:,i]\n",
    "            \n",
    "            self.path_grad[:,i+1] = self.path_grad[:,i]  +  beta * self.path_grad[:,i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path_grad[:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path_grad[:,i] * dt ** 0.5 * dW[1,:,i]\n",
    "            \n",
    "            self.grad[0,:,:,i+1] = self.grad[0,:,:,i]  + beta * self.grad[0,:,:,i] * dt \\\n",
    "                + numpy.transpose(numpy.tile(tspace_fact[i],(BATCH_SIZE,1))) * dt \\\n",
    "                - self.grad[0,:,:,i] * numpy.dot(gamma[1],tspace_fact[i]) * dt \\\n",
    "                + self.grad[0,:,:,i] * numpy.tile(dW[1,:,i],(n,1)) * dt ** 0.5 \n",
    "            \n",
    "            self.grad[1,:,:,i+1] = self.grad[1,:,:,i] + beta * self.grad[1,:,:,i] * dt \\\n",
    "                - numpy.outer(tspace_fact[i],self.path_grad[:,i]) * dt \\\n",
    "                - self.grad[1,:,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "                + self.grad[1,:,:,i] * numpy.tile(dW[1,:,i],(n,1)) * dt ** 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20dc658",
   "metadata": {},
   "source": [
    "#### *Compute Jacobian*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b221af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_jacobian(X, nodes_idx, n_nodes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : an instance of MKVSDEpoly_approx\n",
    "    nodes_idx : indices corresponding to Chebychev nodes\n",
    "    n_nodes: number of Chebychev nodes\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    Jacobian matrix corresponding to polynomial drift model\n",
    "    \"\"\"\n",
    "    jacob1 = numpy.concatenate([numpy.ones(n_nodes), \\\n",
    "                                2 * X.path_grad[tnodes_idx]])\n",
    "    jacob1 = numpy.transpose(numpy.tile(jacob1, (2*n_nodes,1))) \n",
    "    \n",
    "    jacob2_1 = X.grad[0][0][nodes_idx]\n",
    "    jacob2_2 = X.grad[1][0][nodes_idx]\n",
    "    for i in range(n_nodes-1):\n",
    "        jacob2_1 = numpy.vstack([jacob2_1, X.grad[0][i+1][nodes_idx]])\n",
    "        jacob2_2 = numpy.vstack([jacob2_2, X.grad[1][i+1][nodes_idx]])\n",
    "    jacob2_1 = numpy.transpose(jacob2_1)    \n",
    "    jacob2_2 = numpy.transpose(jacob2_2)\n",
    "    jacob2_1 = numpy.vstack([jacob2_1, jacob2_1])\n",
    "    jacob2_2 = numpy.vstack([jacob2_2, jacob2_2])\n",
    "    jacob2 = numpy.hstack([jacob2_1, jacob2_2])\n",
    "    \n",
    "    # element wise product\n",
    "    jacobian = jacob1 * jacob2 \n",
    "    jacobian = jacobian - numpy.eye(2*n_nodes)\n",
    "    \n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9976f",
   "metadata": {},
   "source": [
    "#### *Compute derivative of the L2 loss function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779fe1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_l2loss(X, gamma, wght_mat, nodes_idx, n_nodes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : an instance of MKVSDEpolybatch_approx\n",
    "    gamma : approximation of gamma from previous SGD step\n",
    "    wght_mat : pre-computed weights to calculate the L2 norm\n",
    "    nodes_idx : indices corresponding to Chebychev nodes\n",
    "    n_nodes : number of Chebychev nodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Derivative of the L2 loss function with respect to the gamma value vector\n",
    "    \"\"\"\n",
    "    lossF1 = numpy.transpose(numpy.tile(X.path[tnodes_idx] - gamma[0], (n_nodes,1)))\n",
    "    lossF2 = numpy.transpose(numpy.tile(X.path[tnodes_idx]**2 - gamma[1], (n_nodes,1)))\n",
    "    lossF = numpy.hstack([lossF1, lossF2])\n",
    "    \n",
    "    lossl2F = lossF * wght_mat\n",
    "    \n",
    "    jacobian = poly_jacobian(X, tnodes_idx, n_nodes)\n",
    "    lossl2mat = numpy.matmul(lossl2F, jacobian)\n",
    "    lossl2 = numpy.sum(lossl2mat, 0)\n",
    "    return 2 * lossl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c67361",
   "metadata": {},
   "source": [
    "#### *Compute derivative of the L2 loss function using mini-batch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a142e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  deriv_batchl2loss(X, gamma, wght_mat, nodes_idx, n_nodes, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : a mini-batch of MKVSDEpolybatch_approx\n",
    "    gamma : approximation of gamma from previous SGD step\n",
    "    wght_mat : pre-computed weights to calculate the L2 norm\n",
    "    nodes_idx : indices corresponding to Chebychev nodes\n",
    "    n_nodes: number of Chebychev nodes\n",
    "    BATCH_SIZE: mini-batch size\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Derivative of the L2 loss function with respect to the gamma value vector using mini-batch of paths\n",
    "    \"\"\"\n",
    "    lossl2 = numpy.zeros(2*n_nodes)\n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        jacob1 = numpy.concatenate([numpy.ones(n_nodes), \\\n",
    "                                    2 * X.path_grad[batch_idx, tnodes_idx]])\n",
    "        jacob1 = numpy.transpose(numpy.tile(jacob1, (2*n_nodes,1))) \n",
    "\n",
    "        jacob2_1 = X.grad[0,0,batch_idx,nodes_idx]\n",
    "        jacob2_2 = X.grad[0,0,batch_idx,nodes_idx]\n",
    "        for i in range(n_nodes-1):\n",
    "            jacob2_1 = numpy.vstack([jacob2_1, X.grad[0][i+1][batch_idx][nodes_idx]])\n",
    "            jacob2_2 = numpy.vstack([jacob2_2, X.grad[1][i+1][batch_idx][nodes_idx]])\n",
    "        jacob2_1 = numpy.transpose(jacob2_1)    \n",
    "        jacob2_2 = numpy.transpose(jacob2_2)\n",
    "        jacob2_1 = numpy.vstack([jacob2_1, jacob2_1])\n",
    "        jacob2_2 = numpy.vstack([jacob2_2, jacob2_2])\n",
    "        jacob2 = numpy.hstack([jacob2_1, jacob2_2])\n",
    "\n",
    "        # element wise product\n",
    "        jacobian = jacob1 * jacob2 \n",
    "        jacobian = jacobian - numpy.eye(2*n_nodes)\n",
    "\n",
    "        lossF1 = numpy.transpose(numpy.tile(X.path[batch_idx][tnodes_idx] - gamma[0], (n_nodes,1)))\n",
    "        lossF2 = numpy.transpose(numpy.tile(X.path[batch_idx][tnodes_idx]**2 - gamma[1], (n_nodes,1)))\n",
    "        lossF = numpy.hstack([lossF1, lossF2])\n",
    "        lossl2F = lossF * wght_mat\n",
    "        lossl2mat = numpy.matmul(lossl2F, jacobian)\n",
    "        lossl2 = lossl2 + numpy.sum(lossl2mat, 0)\n",
    "    return lossl2 / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b078fb4",
   "metadata": {},
   "source": [
    "#### *SGD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3141bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# seed initialise\n",
    "random.seed(1881)\n",
    "\n",
    "error = 1000 # not used inside SGD \n",
    "gamma = x0 * numpy.ones((2, n))\n",
    "gamma_SGD = numpy.ones((2, n)) # store final values\n",
    "Z = MKVSDEpoly_approx(tgrid_values, n)\n",
    "gamma_aver = numpy.zeros((2, n))\n",
    "dW = numpy.random.randn(max_iter,2,tgrid_values-1)\n",
    "\n",
    "start_time = time.time()\n",
    "ctr = 0 # iteration counter\n",
    "while ctr < max_iter and error > error_tol:\n",
    "    Z.generatepath(x0, gamma, rho, tgrid_values, tspace, dW[ctr])\n",
    "    gamma_prev = gamma\n",
    "    gamma_prev_SGD = numpy.concatenate([gamma_prev[0], gamma_prev[1]])\n",
    "    gamma_SGD = gamma_prev_SGD - lr * deriv_l2loss(Z, gamma, normwt_mat, tnodes_idx, n)\n",
    "    gamma = numpy.array([gamma_SGD[0:n],gamma_SGD[n:2*n]])\n",
    "    gamma_aver += gamma\n",
    "\n",
    "    if (ctr + 1) % 1000 == 0:\n",
    "        print('Iteration: [{}/{}] '.format(ctr, max_iter))\n",
    "    ctr += 1\n",
    "\n",
    "gamma_SGD = gamma_aver / max_iter\n",
    "\n",
    "SGD_run_time = float(time.time() - start_time)\n",
    "\n",
    "print(\"---Run time for SGD algo: %f seconds ---\" % SGD_run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaedbe7",
   "metadata": {},
   "source": [
    "#### *SGD with mini-batch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28fcdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [1999/5000] \n"
     ]
    }
   ],
   "source": [
    "# seed initialise\n",
    "random.seed(1881)\n",
    "\n",
    "error = 1000 # not used inside SGD \n",
    "gamma = x0 * numpy.ones((2, n))\n",
    "gamma_batchSGD = numpy.ones((2, n)) # store final values\n",
    "Z = MKVSDEpolybatch_approx(tgrid_values, n, BATCH_SIZE)\n",
    "gamma_batchaver = numpy.zeros((2, n))\n",
    "dW = numpy.random.randn(max_batch_iter, 2, BATCH_SIZE, tgrid_values-1)\n",
    "\n",
    "start_time = time.time()\n",
    "ctr = 0 # iteration counter\n",
    "while ctr < max_batch_iter and error > error_tol:\n",
    "    Z.generatebatch(x0, gamma, rho, tgrid_values, n, BATCH_SIZE, tspace, dW[ctr])\n",
    "    loss_grad  = deriv_batchl2loss(Z, gamma, normwt_mat, tnodes_idx, n, BATCH_SIZE)    \n",
    "    gamma_prev = gamma\n",
    "    gamma_prev_SGD = numpy.concatenate([gamma_prev[0], gamma_prev[1]])\n",
    "    gamma_SGD = gamma_prev_SGD - lr * loss_grad    \n",
    "    gamma = numpy.array([gamma_SGD[0:n],gamma_SGD[n:2*n]])\n",
    "    gamma_batchaver += gamma\n",
    "\n",
    "    if (ctr + 1) % (10 * BATCH_SIZE) == 0:\n",
    "        print('Iteration: [{}/{}] '.format(ctr, max_batch_iter))\n",
    "    ctr += 1\n",
    "\n",
    "gamma_batchSGD = gamma_batchaver / max_batch_iter\n",
    "\n",
    "batchSGD_run_time = float(time.time() - start_time)\n",
    "\n",
    "print(\"---Run time for mini-batch SGD algo: %f seconds ---\" % batchSGD_run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077045e",
   "metadata": {},
   "source": [
    "#### *Monte Carlo benchmark and polynomial function approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48daff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 * 10 ** 6\n",
    "ZMC1 = numpy.zeros(N)\n",
    "ZMC1 = x0 # starting value\n",
    "gamma_MC = numpy.zeros((2,tgrid_values))\n",
    "gamma_MC[0, 0] = numpy.mean(ZMC1)\n",
    "gamma_MC[1, 0] = numpy.mean(ZMC1 **2)\n",
    "ZMC2 = numpy.zeros(N)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(tgrid_values-1):\n",
    "    dW = numpy.random.randn(N)\n",
    "    dt = tspace[i+1] - tspace[i]\n",
    "    ZMC2 = ZMC1  + rho * ZMC1 * dt + gamma_MC[0, i] * dt \\\n",
    "    - ZMC1 * gamma_MC[1, i] * dt \\\n",
    "    + ZMC1 * dt ** 0.5 * dW\n",
    "\n",
    "    ZMC1 = ZMC2\n",
    "    gamma_MC[0, i+1] = numpy.mean(ZMC1)\n",
    "    gamma_MC[1, i+1] = numpy.mean(ZMC1 ** 2)\n",
    "\n",
    "MC_run_time = float(time.time() - start_time)\n",
    "print(\"---Run time for MC: %f seconds ---\" % MC_run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3392f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma_approx = numpy.zeros((2, tgrid_values))\n",
    "for i in range(tgrid_values):\n",
    "    gamma_approx[0, i] =  numpy.dot(gamma_batchSGD[0], tspace_fact[i]) \n",
    "    gamma_approx[1, i] =  numpy.dot(gamma_batchSGD[1], tspace_fact[i])\n",
    "    \n",
    "tot_err_gamma1 = (numpy.sum(numpy.abs(gamma_approx[0] - gamma_MC[0]))) / numpy.abs(numpy.mean(gamma_MC[0])) * 100\n",
    "tot_err_gamma2 = (numpy.sum(numpy.abs(gamma_approx[1] - gamma_MC[1]))) / numpy.abs(numpy.mean(gamma_MC[1])) * 100\n",
    "# plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "x_err_bar = numpy.array(['gamma_1','gamma_2'])\n",
    "y_err_bar = numpy.array([tot_err_gamma1,tot_err_gamma2])\n",
    "\n",
    "x_runtime_bar = numpy.array(['batchSGD','MC'])\n",
    "y_runtime_bar = numpy.array([batchSGD_run_time,MC_run_time])\n",
    "\n",
    "ax1.plot(tspace, gamma_approx[0], 'y', label='approx E[Z]')\n",
    "ax1.plot(tspace, gamma_MC[0], 'g', label='MC E[Z]')\n",
    "ax2.plot(tspace, gamma_approx[1], 'r', label='approx E[Z^2]')\n",
    "ax2.plot(tspace, gamma_MC[1], 'b', label='MC E[Z^2]')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "ax3.bar(x_err_bar, y_err_bar, width=0.1, bottom=None, align='center')\n",
    "ax3.set_ylabel('total error (in \\%)')\n",
    "ax4.bar(x_runtime_bar, y_runtime_bar, width=0.1, bottom=None, align='center')\n",
    "ax4.set_ylabel('run time (in seconds)')\n",
    "# plt.rc('text', usetex=False)\n",
    "title_str = 'rho = ' + str(rho) + ', lr = ' + str(lr) + ', BATCH_SIZE = ' + str(BATCH_SIZE) + ', max_batch_iter = ' + str(max_batch_iter) + ', M = ' + str(M) + ', n = ' + str(n) + ', T = ' + str(T)\n",
    "fig.suptitle(title_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
