{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a00fece",
   "metadata": {},
   "source": [
    "## SGD setup for a MKVSDE with polynomial drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ee68b",
   "metadata": {},
   "source": [
    "Consider a McKean-Vlasov (MKV) SDE of the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{d}X_t = \\bigl(\\beta X_t + \\mathbb{E}[X_t] - X_t \\mathbb{E}[X^2_t] \\bigr) \\textrm{d} t + X_t \\, \\textrm{d} W_t, \\quad X_0 = x_0.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca4404",
   "metadata": {},
   "source": [
    "- Method 2: \n",
    "    \n",
    "    We approximate the solution $X_t, t \\in [0,T]$ by using a polynomial approximation for the function $\\mathbb{E}[X_t]$ and $\\mathbb{E}[X^2_t)].$ In particular we use $\\widehat{\\gamma}_{1,k}, k=1,\\ldots,K,$ as the value of $\\mathbb{E}[X_t]$ for $t \\in \\{t_1, \\ldots, t_K\\},$ and $\\widehat{\\gamma}_{2,k}, k=1,\\ldots,K,$ as the value of $\\mathbb{E}[X^2_t]$ for $t \\in \\{t_1, \\ldots, t_K\\},$ where the values $\\{t_1, \\ldots, t_K\\},$ are either Chebychev or Lagrange nodes in the time interval $[0, T].$ The process with polynomial function approximation is generated as:\n",
    "    \n",
    "    \\begin{equation}\n",
    "    \\textrm{d}Z_t = \\bigl(\\beta Z_t + \\sum^{K}_{k=1} \\widehat{\\gamma}_{1,k} \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  -  Z_t \\sum^{K}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}  \\bigr) \\textrm{d} t + Z_t \\, \\textrm{d} W_t, \\quad Z_0 = x_0.\n",
    "    \\end{equation}\n",
    "    \n",
    "    The gradient processes $\\xi^k_{1,t}:= \\partial_{\\widehat{\\gamma}_{1,k}}Z_t$ and $\\xi^k_{2,t}:= \\partial_{\\widehat{\\gamma}_{2,k}}Z_t$ for $k = 1, \\ldots, K,$ are given as \n",
    "    \n",
    "    \\begin{align}\n",
    "    \\textrm{d}\\xi^k_{1,t} &= \\bigl(\\beta \\xi^k_{1,t} + \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}-\\xi^k_{1,t} \\sum^{K}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}\\bigr) \\textrm{d} t + \\xi^k_{1,t}\\, \\textrm{d} W_t, \\quad \\xi^k_{1,0} = 0,\\\\\n",
    "    \\textrm{d}\\xi^k_{2,t} &= \\bigl(\\beta \\xi^k_{2,t} -\\xi^k_{2,t} \\sum^{K}_{k=1} \\widehat{\\gamma}_{2,k} \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j} - Z_t \\prod_{\\substack{ 1\\leq j \\leq K \\\\ k\\neq j}} \\frac{t-t_j}{t_k - t_j}\\bigr) \\textrm{d} t  + \\xi^k_{2,t}\\, \\textrm{d} W_t, \\quad \\xi^k_{2,0} = 0.\n",
    "    \\end{align}\n",
    "    \n",
    "    We disretise the time interval $[0, T]$ uniformly into $M + 1$ steps, and insert Chebychev nodes to create a final time grid with $M+K$ intermediate points to generate discretised path of $Z$ and gradient processes. Our aim is to find the fixed-point of the map $\\widehat{\\Psi}^{(M+K)}$ defined as\n",
    "    \\begin{equation}\n",
    "      \\widehat{\\Psi}^{(M+K)}(\\widehat{\\gamma}):= \\mathcal{P}\\Bigl(\\Psi\\bigl(\\mathcal{L}(\\widehat{\\gamma})\\bigr)\\Bigr).\n",
    "    \\end{equation}\n",
    "    \n",
    "    In the linear MKV-SDE considered here, we have $\\Psi\\bigl(\\mathcal{L}(\\widehat{\\gamma})\\bigr)(t) = \\bigl(\\mathbb{E}[Z_t], \\mathbb{E}[Z^2_t]\\bigr).$ We compute the fixed-point by solving the following: \n",
    "    \\begin{equation}\n",
    "   \\min_{\\widehat{\\gamma}} \\big| \\widehat{\\gamma} - \\widehat{\\Psi}^{(M+K)}(\\widehat{\\gamma})\\big|^2.\n",
    "    \\end{equation}\n",
    "    \n",
    "    To perform the numerical step, we vectorize $\\widehat{\\gamma}$ and $\\widehat{\\Psi}^{(M+K)}(\\widehat{\\gamma})$ by concatenating the rows to create a vector. Thus $\\widehat{\\gamma} = (\\widehat{\\gamma}_{1,1}, \\ldots, \\widehat{\\gamma}_{1,K}, \\widehat{\\gamma}_{2,1}, \\ldots, \\widehat{\\gamma}_{2,K}).$\n",
    "    \n",
    "    \\begin{equation}\n",
    "    F_i(\\widehat{\\gamma}, W) = Z_{t_i} - \\widehat{\\gamma}_{1,i}, i=1, \\ldots, K, \\quad \\text{and,} \\quad F_i(\\widehat{\\gamma}, W) = Z^2_{t_{i-K}} - \\widehat{\\gamma}_{2,{i-K}}, i=K+1, \\ldots, 2K.\n",
    "    \\end{equation}\n",
    "    \n",
    "    The Jacobian matrix is then given as \n",
    "    \\begin{equation}\n",
    "        J_{\\widehat{\\gamma}}F = \\begin{bmatrix}\n",
    "                                \\nabla^\\top F_1(\\widehat{\\gamma}, W) \\\\\n",
    "                                \\vdots\\\\\n",
    "                                \\nabla^\\top F_{2K}(\\widehat{\\gamma}, W)\n",
    "                                \\end{bmatrix}\n",
    "                                = \\begin{bmatrix}\n",
    "                                    \\frac{\\partial F_1}{\\partial \\widehat{\\gamma}_{1,1}} \\ldots \\frac{\\partial F_1}{\\partial \\widehat{\\gamma}_{2,K}}\\\\\n",
    "                                    \\vdots \\ddots \\vdots \\\\\n",
    "                                    \\frac{\\partial F_{2K}}{\\partial \\widehat{\\gamma}_{1,1}} \\ldots \\frac{\\partial F_{2K}}{\\partial \\widehat{\\gamma}_{2,K}}\\\\\n",
    "                                    \\end{bmatrix}\n",
    "                               = \\begin{bmatrix}\n",
    "                                   \\xi^1_{1,t_1} - 1 & \\xi^2_{1,t_1} & \\ldots & \\xi^{K}_{2,t_1}\\\\\n",
    "                                   \\xi^1_{1,t_2} & \\xi^2_{1,t_2} - 1 & \\ldots & \\xi^K_{2,t_2}\\\\\n",
    "                                    \\vdots & \\vdots & \\ldots & \\vdots \\\\\n",
    "                                   2 Z_{t_K} \\xi^1_{1,t_K} & 2 Z_{t_K} \\xi^2_{1,t_K} & \\ldots & 2 Z_{t_K}  \\xi^K_{2,t_K} - 1\n",
    "                               \\end{bmatrix}.\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5752a34",
   "metadata": {},
   "source": [
    "To the loss function, one can also add a constraint $\\gamma_2 = \\mathbb{E}[Z^2_t] \\geq (\\mathbb{E}[Z])^2 = (\\gamma_1)^2$ to ensure that $\\gamma_2$ remains positive and there is some relation between the two quantities. Thus, our loss function is given as \n",
    "\\begin{equation}\n",
    "\\min_{\\widehat{\\gamma}} \\big| \\widehat{\\gamma} - \\widehat{\\Psi}^{(M+K)}(\\widehat{\\gamma})\\big|^2 + \\lambda_{pen}\\sum^K_{k=1}\\bigl((\\widehat{\\gamma}_{1,k})^2 - \\widehat{\\gamma}_{2,k}\\bigr).\n",
    "\\end{equation}\n",
    " \n",
    "When differentiating the above penalization function w.r.t. $\\gamma_{1,k}$, we get a term of \n",
    "\\begin{equation}\n",
    "\\lambda_{pen} \\times 2 \\times \\gamma_{1,k},\n",
    "\\end{equation}\n",
    "and when differentiating the above penalization function w.r.t. $\\gamma_{2,k}$, we get a term of \n",
    "\\begin{equation}\n",
    "- \\lambda_{pen}.\n",
    "\\end{equation}\n",
    "Thus, the update step of SGD algorithm needs to be modified to reflect the constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b5e7b",
   "metadata": {},
   "source": [
    "#### *Things to be added*\n",
    "\n",
    "- include anti-thetic variates\n",
    "- include adaptive learning rate\n",
    "- clipping the gradient based if its norm exceeds a bound\n",
    "- increase $T$ somewhere to be $100$\n",
    "- use a weighted $L2$-norm for minimisation: scale each squared Euclidean difference with a weight of the type $\\mathrm{e}^{-\\alpha t_k}$\n",
    "- include a constraint on $\\gamma_2 > (\\gamma_1)^2$ (*try last*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778eb49",
   "metadata": {},
   "source": [
    "### Implementation of method with approximation using Chebychev nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902dc38c",
   "metadata": {},
   "source": [
    "#### *Import relevant libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e646d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rc # for TeX in plot labels\n",
    "\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "## for Palatino and other serif fonts use:\n",
    "# rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "# rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f910e0",
   "metadata": {},
   "source": [
    "#### *Import plot libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fff3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1a30b",
   "metadata": {},
   "source": [
    "#### *Chebyvchev nodes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CC_nodes(a, b, n):\n",
    "    i = numpy.array(range(n))\n",
    "    x = numpy.cos((2 * i + 1) * pi /(2 * n))\n",
    "    z = 0.5 * (b-a) * x + 0.5 * (b + a)\n",
    "    return numpy.flip(z, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5f11d",
   "metadata": {},
   "source": [
    "#### *Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466cf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100  # number of discretisation steps\n",
    "K = 5  # number of Chebychev nodes\n",
    "x0 = 1\n",
    "beta = -0.5\n",
    "T = 1.0\n",
    "dt = T / M\n",
    "max_iter = 5 * 10 ** 4\n",
    "max_batch_iter = 10 ** 4\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0a15b",
   "metadata": {},
   "source": [
    "#### *Compute time grid and factors in polynomial approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39d102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspace = numpy.linspace(0, T, M + 1, dtype=float) # discretisation points\n",
    "tnodes = CC_nodes(0, T, K) # Chebychev nodes\n",
    "\n",
    "# insert unique Chebychev nodes in the time grid based on M values\n",
    "tspace = numpy.unique(numpy.concatenate((tspace, tnodes), axis=0))\n",
    "tgrid_values = len(tspace)\n",
    "\n",
    "# identify the indices of Chebychev nodes in the time grid\n",
    "tnodes_idx = numpy.zeros(K, dtype=int)\n",
    "ctr = 0\n",
    "for j in range(tgrid_values):\n",
    "    if ctr < K: \n",
    "        if tnodes[ctr] == tspace[j]:\n",
    "            tnodes_idx[ctr] = j\n",
    "            ctr +=1 \n",
    "\n",
    "# tgrid_values product values for K different Chebychev nodes\n",
    "tspace_fact = numpy.ones((tgrid_values, K)) \n",
    "tnode_prod = numpy.zeros(K) # for computing the denominator in the factor\n",
    "\n",
    "# compute products in the denominator of polynomial approxmiation factor\n",
    "for k in range(K):\n",
    "    temp_prod = tnodes[k] - numpy.delete(tnodes, k)\n",
    "    tnode_prod[k] = numpy.prod(temp_prod)\n",
    "\n",
    "# compute time factors in the polynomial approxmiation\n",
    "for i in range(tgrid_values):\n",
    "    for k in range(K):\n",
    "        temp_prod = tspace[i] - numpy.delete(tnodes, k)\n",
    "        tspace_fact[i][k] =  numpy.prod(temp_prod) / tnode_prod[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b3cf7",
   "metadata": {},
   "source": [
    "#### *Define MKV SDE class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d7f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MKVSDEpoly_approx():\n",
    "    \"\"\"\n",
    "    An MKVSDE class with 3 types of paths:\n",
    "        1. Underlying MKV path for computing the loss function with BM 1\n",
    "        2. An independent MKV path for computing the jacobian with BM 2\n",
    "        3. Gradient process paths driven by BM 2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tgrid_values, K):\n",
    "        super().__init__()\n",
    "        self.path = numpy.zeros(tgrid_values)\n",
    "        self.path_grad = numpy.zeros(tgrid_values) # sample path to be used for gradient process\n",
    "        self.grad = numpy.zeros((2, K, tgrid_values)) # gradient processes\n",
    "    \n",
    "    def generatepath(self, x0, gamma, beta, tgrid_values, tspace, dW):\n",
    "        self.path[0] = x0\n",
    "        self.path_grad[0] = x0\n",
    "        for i in range(tgrid_values-1):\n",
    "            dt = tspace[i+1] - tspace[i]\n",
    "            \n",
    "            self.path[i+1] = self.path[i]  +  beta * self.path[i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path[i] * dt ** 0.5 * dW[0][i]\n",
    "            \n",
    "            self.path_grad[i+1] = self.path_grad[i]  +  beta * self.path_grad[i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path_grad[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path_grad[i] * dt ** 0.5 * dW[1][i]\n",
    "            \n",
    "            self.grad[0,:,i+1] = self.grad[0,:,i]  + beta * self.grad[0,:,i] * dt \\\n",
    "                + tspace_fact[i] * dt \\\n",
    "                - self.grad[0,:,i] * numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "                + self.grad[0,:,i] * dt ** 0.5 * dW[1][i]\n",
    "            \n",
    "            self.grad[1,:,i+1] = self.grad[1,:,i] + beta * self.grad[1,:,i] * dt \\\n",
    "                - self.path_grad[i] * tspace_fact[i] * dt \\\n",
    "                - self.grad[1,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "                + self.grad[1,:,i] * dt ** 0.5 * dW[1][i]\n",
    "                \n",
    "class MKVSDEpolybatch_approx():\n",
    "    \"\"\"\n",
    "    An MKVSDE class that has a batch of size BATCH_SIZE with 3 types of paths:\n",
    "        1. Underlying MKV path for computing the loss function with BM 1\n",
    "        2. An independent MKV path for computing the jacobian with BM 2\n",
    "        3. Gradient process paths driven by BM 2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tgrid_values, K, BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.path = numpy.zeros((BATCH_SIZE, tgrid_values))\n",
    "        self.path_grad = numpy.zeros((BATCH_SIZE, tgrid_values)) # sample path to be used for gradient process\n",
    "        self.grad = numpy.zeros((2, K, BATCH_SIZE, tgrid_values)) # gradient processes\n",
    "    \n",
    "    def generatebatch(self, x0, gamma, beta, tgrid_values, K, BATCH_SIZE, tspace, dW):\n",
    "        self.path[:,0] = x0\n",
    "        self.path_grad[:,0] = x0\n",
    "        for i in range(tgrid_values-1):\n",
    "            dt = tspace[i+1] - tspace[i]\n",
    "            \n",
    "            self.path[:,i+1] = self.path[:,i]  +  beta * self.path[:,i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path[:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path[:,i] * dt ** 0.5 * dW[0,:,i]\n",
    "            \n",
    "            self.path_grad[:,i+1] = self.path_grad[:,i]  +  beta * self.path_grad[:,i] * dt \\\n",
    "            + numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
    "            - self.path_grad[:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "            + self.path_grad[:,i] * dt ** 0.5 * dW[1,:,i]\n",
    "            \n",
    "            self.grad[0,:,:,i+1] = self.grad[0,:,:,i]  + beta * self.grad[0,:,:,i] * dt \\\n",
    "                + numpy.transpose(numpy.tile(tspace_fact[i],(BATCH_SIZE,1))) * dt \\\n",
    "                - self.grad[0,:,:,i] * numpy.dot(gamma[0],tspace_fact[i]) * dt \\\n",
    "                + self.grad[0,:,:,i] * numpy.tile(dW[1,:,i],(K,1)) * dt ** 0.5 \n",
    "            \n",
    "            self.grad[1,:,:,i+1] = self.grad[1,:,:,i] + beta * self.grad[1,:,:,i] * dt \\\n",
    "                - numpy.outer(tspace_fact[i],self.path_grad[:,i]) * dt \\\n",
    "                - self.grad[1,:,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
    "                + self.grad[1,:,:,i] * numpy.tile(dW[1,:,i],(K,1)) * dt ** 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20dc658",
   "metadata": {},
   "source": [
    "#### *Compute Jacobian*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b221af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_jacobian(X, nodes_idx, n_nodes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : an instance of MKVSDEpoly_approx\n",
    "    nodes_idx : indices corresponding to Chebychev nodes\n",
    "    n_nodes: number of Chebychev nodes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Jacobian matrix corresponding to polynomial drift model\n",
    "    \"\"\"\n",
    "    # matrix of basis function at independent path values\n",
    "    jacob1 = numpy.concatenate([numpy.ones(n_nodes), \\\n",
    "                                2 * X.path_grad[tnodes_idx]])\n",
    "    jacob1 = numpy.transpose(numpy.tile(jacob1, (2*n_nodes,1))) \n",
    "    \n",
    "    # final matrix of gradient values -- This part needs to be generalised and optimised\n",
    "    jacob2_1 = X.grad[0][0][nodes_idx]\n",
    "    jacob2_2 = X.grad[1][0][nodes_idx]\n",
    "    for i in range(n_nodes-1):\n",
    "        jacob2_1 = numpy.vstack([jacob2_1, X.grad[0][i+1][nodes_idx]])\n",
    "        jacob2_2 = numpy.vstack([jacob2_2, X.grad[1][i+1][nodes_idx]])\n",
    "    jacob2_1 = numpy.transpose(jacob2_1)    \n",
    "    jacob2_2 = numpy.transpose(jacob2_2)\n",
    "    jacob2_1 = numpy.vstack([jacob2_1, jacob2_1])\n",
    "    jacob2_2 = numpy.vstack([jacob2_2, jacob2_2])\n",
    "    jacob2 = numpy.hstack([jacob2_1, jacob2_2])\n",
    "    \n",
    "    # element wise product\n",
    "    jacobian = jacob1 * jacob2 \n",
    "    jacobian = jacobian - numpy.eye(2*n_nodes)\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "def poly_batch_jacobian(X, nodes_idx, n_nodes, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : an instance of MKVSDEpolybatch_approx\n",
    "    nodes_idx : indices corresponding to Chebychev nodes\n",
    "    n_nodes: number of Chebychev nodes\n",
    "    BATCH_SIZE: mini-batch size\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Jacobian matrix corresponding to polynomial drift model using mini-batch\n",
    "    \"\"\"\n",
    "    jacobian_final = numpy.zeros((2*n_nodes,2*n_nodes))\n",
    "    for batch_idx in range(BATCH_SIZE):\n",
    "        # matrix of basis function at independent path values\n",
    "        jacob1 = numpy.concatenate([numpy.ones(n_nodes), \\\n",
    "                                   2 * X.path_grad[batch_idx, tnodes_idx]])\n",
    "        jacob1 = numpy.transpose(numpy.tile(jacob1, (2*n_nodes,1))) \n",
    "\n",
    "        # final matrix of gradient values -- This part needs to be generalised and optimised\n",
    "        jacob2_1 = X.grad[0,0,batch_idx,nodes_idx]\n",
    "        jacob2_2 = X.grad[0,0,batch_idx,nodes_idx]\n",
    "        for i in range(n_nodes-1):\n",
    "            jacob2_1 = numpy.vstack([jacob2_1, X.grad[0][i+1][batch_idx][nodes_idx]])\n",
    "            jacob2_2 = numpy.vstack([jacob2_2, X.grad[1][i+1][batch_idx][nodes_idx]])\n",
    "        jacob2_1 = numpy.transpose(jacob2_1)    \n",
    "        jacob2_2 = numpy.transpose(jacob2_2)\n",
    "        jacob2_1 = numpy.vstack([jacob2_1, jacob2_1])\n",
    "        jacob2_2 = numpy.vstack([jacob2_2, jacob2_2])\n",
    "        jacob2 = numpy.hstack([jacob2_1, jacob2_2])\n",
    "\n",
    "        # element wise product\n",
    "        jacobian = jacob1 * jacob2 \n",
    "        jacobian = jacobian - numpy.eye(2*n_nodes)\n",
    "        jacobian_final = jacobian_final + jacobian\n",
    "        \n",
    "    return jacobian_final / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b078fb4",
   "metadata": {},
   "source": [
    "#### *SGD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3141bc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [999/50000] \n",
      "Iteration: [1999/50000] \n",
      "Iteration: [2999/50000] \n",
      "Iteration: [3999/50000] \n",
      "Iteration: [4999/50000] \n",
      "Iteration: [5999/50000] \n",
      "Iteration: [6999/50000] \n",
      "Iteration: [7999/50000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:23: RuntimeWarning: overflow encountered in double_scalars\n",
      "  - self.path[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:28: RuntimeWarning: overflow encountered in double_scalars\n",
      "  - self.path_grad[i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.path[i+1] = self.path[i]  +  beta * self.path[i] * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self.path_grad[i+1] = self.path_grad[i]  +  beta * self.path_grad[i] * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:33: RuntimeWarning: overflow encountered in multiply\n",
      "  - self.grad[0,:,i] * numpy.dot(gamma[0], tspace_fact[i]) * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:38: RuntimeWarning: overflow encountered in multiply\n",
      "  - self.grad[1,:,i] * numpy.dot(gamma[1], tspace_fact[i]) * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:31: RuntimeWarning: invalid value encountered in add\n",
      "  self.grad[0,:,i+1] = self.grad[0,:,i]  + beta * self.grad[0,:,i] * dt \\\n",
      "C:\\Users\\aa261w\\AppData\\Local\\Temp/ipykernel_13796/2364043571.py:36: RuntimeWarning: invalid value encountered in add\n",
      "  self.grad[1,:,i+1] = self.grad[1,:,i] + beta * self.grad[1,:,i] * dt \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: [8999/50000] \n",
      "Iteration: [9999/50000] \n",
      "Iteration: [10999/50000] \n",
      "Iteration: [11999/50000] \n",
      "Iteration: [12999/50000] \n",
      "Iteration: [13999/50000] \n",
      "Iteration: [14999/50000] \n",
      "Iteration: [15999/50000] \n",
      "Iteration: [16999/50000] \n",
      "Iteration: [17999/50000] \n",
      "Iteration: [18999/50000] \n",
      "Iteration: [19999/50000] \n",
      "Iteration: [20999/50000] \n",
      "Iteration: [21999/50000] \n",
      "Iteration: [22999/50000] \n",
      "Iteration: [23999/50000] \n",
      "Iteration: [24999/50000] \n",
      "Iteration: [25999/50000] \n",
      "Iteration: [26999/50000] \n",
      "Iteration: [27999/50000] \n",
      "Iteration: [28999/50000] \n",
      "Iteration: [29999/50000] \n",
      "Iteration: [30999/50000] \n",
      "Iteration: [31999/50000] \n",
      "Iteration: [32999/50000] \n",
      "Iteration: [33999/50000] \n",
      "Iteration: [34999/50000] \n",
      "Iteration: [35999/50000] \n",
      "Iteration: [36999/50000] \n",
      "Iteration: [37999/50000] \n"
     ]
    }
   ],
   "source": [
    "# seed initialise\n",
    "random.seed(1881)\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "l2pen = 0  # l2 penalization parameter\n",
    "lpen = 0 # parameter for constraint between gamma_1 and gamma_2\n",
    "error_tol = 10 ** -5 # error tolerance\n",
    "error = 1000 # not used inside SGD \n",
    "gamma = x0 * numpy.ones((2,K))\n",
    "gamma_SGD = numpy.ones((2,K)) # store final values\n",
    "Z = MKVSDEpoly_approx(tgrid_values, K)\n",
    "gamma_aver = numpy.zeros((2,K))\n",
    "dW = numpy.random.randn(max_iter,2,tgrid_values-1)\n",
    "jacobian = numpy.zeros((2*K,2*K))\n",
    "\n",
    "start_time = time.time()\n",
    "ctr = 0 # iteration counter\n",
    "while ctr < max_iter and error > error_tol:\n",
    "    Z.generatepath(x0, gamma, beta, tgrid_values, tspace, dW[ctr])\n",
    "    loss_grad = 2 * (numpy.concatenate([Z.path[tnodes_idx], Z.path[tnodes_idx] ** 2]) \\\n",
    "                     - numpy.concatenate([gamma[0], gamma[1]]))\n",
    "    \n",
    "    jacobian = poly_jacobian(Z, tnodes_idx, K)\n",
    "    gamma_prev = gamma\n",
    "    gamma_prev_SGD = numpy.concatenate([gamma_prev[0], gamma_prev[1]])\n",
    "#     gamma_prev_SGD_pen = numpy.concatenate([2 * gamma_prev[0], -1.0 * numpy.ones(K)])\n",
    "#     gamma_SGD = gamma_prev_SGD * (1 - l2pen) - lr * numpy.matmul(loss_grad, jacobian) + lpen * gamma_prev_SGD_pen\n",
    "    gamma_SGD = gamma_prev_SGD - lr * numpy.matmul(loss_grad, jacobian) \n",
    "    gamma = numpy.array([gamma_SGD[0:K],gamma_SGD[K:2*K]])\n",
    "    gamma_aver += gamma\n",
    "\n",
    "    if (ctr + 1) % 1000 == 0:\n",
    "        print('Iteration: [{}/{}] '.format(ctr, max_iter))\n",
    "#         lpen = lpen / 1.1\n",
    "    ctr += 1\n",
    "\n",
    "gamma_SGD = gamma_aver / max_iter\n",
    "\n",
    "SGD_run_time = float(time.time() - start_time)\n",
    "\n",
    "print(\"---Run time for SGD algo: %f seconds ---\" % SGD_run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009939a",
   "metadata": {},
   "source": [
    "#### *Compute gradient of loss function with mini-batch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradloss(X, gamma, BATCH_SIZE, nodes_idx):\n",
    "    return 2 * numpy.mean((numpy.concatenate([X.path[:, nodes_idx], X.path[:, nodes_idx] ** 2], axis=1) \\\n",
    "                     - numpy.concatenate([gamma[0], gamma[1]])), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96d58d",
   "metadata": {},
   "source": [
    "#### *SGD with mini-batch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f46c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed initialise\n",
    "random.seed(1881)\n",
    "\n",
    "lr = 0.03 # learning rate\n",
    "error_tol = 10 ** -5 # error tolerance\n",
    "error = 1000 # not used inside SGD \n",
    "gamma = x0 * numpy.ones((2,K))\n",
    "gamma_batchSGD = numpy.ones((2,K)) # store final values\n",
    "Z = MKVSDEpolybatch_approx(tgrid_values, K, BATCH_SIZE)\n",
    "gamma_batchaver = numpy.zeros((2,K))\n",
    "dW = numpy.random.randn(max_batch_iter, 2, BATCH_SIZE, tgrid_values-1)\n",
    "jacobian = numpy.zeros((2*K,2*K))\n",
    "\n",
    "start_time = time.time()\n",
    "ctr = 0 # iteration counter\n",
    "while ctr < max_batch_iter and error > error_tol:\n",
    "    Z.generatebatch(x0, gamma, beta, tgrid_values, K, BATCH_SIZE, tspace, dW[ctr])\n",
    "    loss_grad = batch_gradloss(Z, gamma, BATCH_SIZE, tnodes_idx)\n",
    "    jacobian = poly_batch_jacobian(Z, tnodes_idx, K, BATCH_SIZE)\n",
    "    \n",
    "    gamma_prev = gamma\n",
    "    gamma_prev_SGD = numpy.concatenate([gamma_prev[0], gamma_prev[1]])\n",
    "    gamma_SGD = gamma_prev_SGD - lr * numpy.matmul(loss_grad, jacobian)\n",
    "    \n",
    "    gamma = numpy.array([gamma_SGD[0:K],gamma_SGD[K:2*K]])\n",
    "    gamma_batchaver += gamma\n",
    "\n",
    "    if (ctr + 1) % 500 == 0:\n",
    "        print('Iteration: [{}/{}] '.format(ctr, max_batch_iter))\n",
    "    ctr += 1\n",
    "\n",
    "gamma_batchSGD = gamma_batchaver / max_batch_iter\n",
    "\n",
    "batchSGD_run_time = float(time.time() - start_time)\n",
    "\n",
    "print(\"---Run time for mini-batch SGD algo: %f seconds ---\" % batchSGD_run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077045e",
   "metadata": {},
   "source": [
    "#### *Monte Carlo benchmark and polynomial function approximation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48daff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 * 10 ** 6\n",
    "ZMC1 = numpy.zeros(N)\n",
    "ZMC1 = x0 # starting value\n",
    "gamma_MC = numpy.zeros((2,tgrid_values))\n",
    "gamma_MC[0, 0] = numpy.mean(ZMC1)\n",
    "gamma_MC[1, 0] = numpy.mean(ZMC1 **2)\n",
    "ZMC2 = numpy.zeros(N)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(tgrid_values-1):\n",
    "    dW = numpy.random.randn(N)\n",
    "    dt = tspace[i+1] - tspace[i]\n",
    "    ZMC2 = ZMC1  + beta * ZMC1 * dt + gamma_MC[0, i] * dt \\\n",
    "    - ZMC1 * gamma_MC[1, i] * dt \\\n",
    "    + ZMC1 * dt ** 0.5 * dW\n",
    "\n",
    "    ZMC1 = ZMC2\n",
    "    gamma_MC[0, i+1] = numpy.mean(ZMC1)\n",
    "    gamma_MC[1, i+1] = numpy.mean(ZMC1 ** 2)\n",
    "\n",
    "MC_run_time = float(time.time() - start_time)\n",
    "print(\"---Run time for MC: %f seconds ---\" % MC_run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3392f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma_approx = numpy.zeros((2, tgrid_values))\n",
    "for i in range(tgrid_values):\n",
    "    gamma_approx[0, i] =  numpy.dot(gamma_SGD[0], tspace_fact[i]) \n",
    "    gamma_approx[1, i] =  numpy.dot(gamma_SGD[1], tspace_fact[i])\n",
    "    \n",
    "tot_err_gamma1 = (numpy.sum(numpy.abs(gamma_approx[0] - gamma_MC[0]))) * numpy.abs(numpy.mean(gamma_MC[0])) * 100\n",
    "tot_err_gamma2 = (numpy.sum(numpy.abs(gamma_approx[1] - gamma_MC[1]))) * numpy.abs(numpy.mean(gamma_MC[1])) * 100\n",
    "# plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "x_err_bar = numpy.array(['gamma_1','gamma_2'])\n",
    "y_err_bar = numpy.array([tot_err_gamma1,tot_err_gamma2])\n",
    "\n",
    "x_runtime_bar = numpy.array(['SGD','MC'])\n",
    "y_runtime_bar = numpy.array([SGD_run_time,MC_run_time])\n",
    "\n",
    "ax1.plot(tspace, gamma_approx[0], 'y', label='approx E[Z]')\n",
    "ax1.plot(tspace, gamma_MC[0], 'g', label='MC E[Z]')\n",
    "ax2.plot(tspace, gamma_approx[1], 'r', label='approx E[Z^2]')\n",
    "ax2.plot(tspace, gamma_MC[1], 'b', label='MC E[Z^2]')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "ax3.bar(x_err_bar, y_err_bar, width=0.1, bottom=None, align='center')\n",
    "ax3.set_ylabel('total error (in \\%)')\n",
    "ax4.bar(x_runtime_bar, y_runtime_bar, width=0.1, bottom=None, align='center')\n",
    "ax4.set_ylabel('run time (in seconds)')\n",
    "# plt.rc('text', usetex=False)\n",
    "title_str = 'beta = ' + str(beta) + ', lr = ' + str(lr) + ', BATCH_SIZE = ' + str(BATCH_SIZE) + ', max_batch_iter = ' + str(max_batch_iter) + ', M = ' + str(M) + ', K = ' + str(K) + ', T = ' + str(T)\n",
    "fig.suptitle(title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma = x0 * numpy.ones((2,K))\n",
    "# Z = MKVSDEpolybatch_approx(tgrid_values, K, BATCH_SIZE)\n",
    "# dW = numpy.random.randn(2, 2, BATCH_SIZE, tgrid_values-1)\n",
    "# Z.generatebatch(x0, gamma, beta, tgrid_values, K, BATCH_SIZE, tspace, dW[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
